\section{Элементы машинного обучения} 
Здесь будут приведены некоторые теоретические сведения из анализа данных и машинного обучения. За более подробным материалом можно обратиться к \cite{bishop, mathforml}.

\subsection{Введение в машинное обучение}
{\it Машинное обучение} -- раздел, стоящий на пересечении математики и компьютерных наук, который главным образом решает задачу восстановления некоторой зависимости, используя некоторые данные, которые представляют такую зависимость. 

Машинное обучение можно разделить на категории: например, очень популярное разделение -- это разделение по типу обучения:
\begin{itemize}
	\item Обучение с учителем(Supervised Learning)
	\item Обучение без учителя(Unsupervised Learning)
	\item Обучение с подкреплением(Reinforcement Learning)
\end{itemize}

Остановимся поподробнее на обучении с учителем. Данный вид машинного обучения предполагает, что имеется некоторая выборка $X$ -- {\it множество объектов}, некоторая выборка $Y$ -- {\it множество ответов}, и необходимо восстановить {\it скрытую зависимость} $y: X \to Y$. Как правило, саму зависимость восстановить порой очень трудно, поэтому достаточно найти такой алгоритм $a: X \to Y$, который приближает $y$ на $X$. При этом сами объекты задаются через {\it признаки} $f: X \to D$, где $D$ -- {\it пространство признаков}. Признаки -- это какие-либо характеристики, которые описывают данный объект. 

Основные задачи обучения с учителем:
\begin{itemize}
	\item Регрессия
	\item Классификация
\end{itemize}

Фундаментальная разница между этими двумя типами задач следующая: задача {\it регрессии} -- восстановление непрерывной функции, которая характеризует скрытую зависимость между входными переменными и выходными. Регрессия таким имеет возможность предсказывать численное значение этой скрытой непрерывной функции по новым значениям входных переменных. Задача {\it классификации} же в том, чтобы восстановить зависимость между входными параметрами, описывающие объект, и тем, к какому классу данный объект принадлежит. То есть в случае задачи классификации скрытая зависимость описывается некоторой пороговой функцией, где каждое значение области значения соответствует некоторому классу.

Важным шагом в решении задачи как регрессии, так и классификации, является выбор модели и настройка ее параметров. {\it Модель} -- это параметрическое семейство функций 
\[
	A \coloneqq \{ a(x) = g(x, \theta) | \theta \in \Theta \},
\]
где $\Theta$ -- множество значений параметра $\theta$, а $g: X \times \Theta \to Y$ -- некоторая фиксированная функция.

Решение задачи машинного обучения разделяется на два этапа:
\begin{enumerate}
	\item Этап обучения
	\item Этап применения
\end{enumerate}

Этап обучения представляет из себя построение алгоритма выбора подходящей модели, т.е. имеется отображение $\mu: X \times Y \to A$, которое по каждой выборке $(X,Y)$, которую обычно называют {\it обучающей выборкой}, ставит в соответствие алгоритм $a \coloneqq \mu(X, Y)$, который наилучшим образом описывает скрытую зависимость $y: X \to Y$. Для выбора модели необходимо сравнивать точность, которую они демонстрируют. Для этого обычно вводят т.н. {\it функцию потерь(loss function)} $L(a,x)$, которая измеряет величину ошибки алгоритма $a \in A$ на объекте $x \in X$. Например для задачи классификации рассматривают {\it индикатор ошибки(Accuracy)}:
\[
	L(a,y,x) = [a(x) \neq y(x)].
\]
А для задач регрессии в качестве функции потерь рассматривают {\it квадратичную ошибку} 
\[
	L(a,y,x) = (a(x) - y(x))^2,
\]
или {\it абсолютное значение ошибки}
\[
	L(a,y,x) = \abs{a(x) - y(x)}.
\]
Чтобы оценить качество работы алгоритма $a$ на всей выборке $X$ обычно считают среднее арифметическое по значениям функции потерь для каждого объекта $x \in X$(иногда это называют {\it эмпирическим риском}):
\[
	Q(a,X, Y) = \frac{1}{\abs{X}}\sum\limits_{i=1}^{\abs{X}} L(a,y_i, x_i).
\]
И тогда для выбора модели решают задачу минимизации эмпирического риска:
\[
	\mu(X, Y) = \arg\min\limits_{a \in A} Q(a, X, Y).
\]


Этап применения -- это следующий этап, который представляет собой т.н. задачу вывода. На этом этапе выбранный алгоритм $a$ для новых объектов $X^*$, которые не были задействованы в этапе обучения, выдает ответы $a(X^*)$. Обычно такую выборку $X^*$ называют {\it тестовой выборкой}. Одной из основных проблем, которые возникают на данном этапе -- это проблема {\it переобучения}. Переобучение -- это ситуация, которая возникает в следствие допущенной ошибки в ходе этапа обучения, и характеризуется тем, что величина ошибки на тестовой выборке сильно превышает величину ошибки на обучающей:
\[
	Q(\mu(X, Y), X^*, Y^*) >> Q(\mu(X, Y), X, Y).
\]
Зачастую такую проблему можно исправить, если правильно подобрать параметры модели, которая использовалась для обучения.

Так как данная работа представляет собой решение задачи классификации, то остановимся поподробнее на соответствующих моделях машинного обучения.

\subsection{Логистическая регрессия}
Логистическая регрессия -- один из самых простых и популярных алгоритмов для задачи классификации. Смысл данного метода заключается в том, чтобы построить разделяющую гиперплоскость в пространстве признаков, позволяющую отделить классы друг от друга. 
\newline
В логистической регрессии строится линейный алгоритм классификации $ a:X \to Y $, где $X$ -- пространство признаков, а $Y$ -- конечное множество номеров классов. Алгоритм имеет вид:
\[
a(x,w) = sign(\sum_{j=1}^{n}w_jf_j(x)-w_0) = sign\langle x,w\rangle,
\]
где $w$ -- вектор весов, $w_0$ -- порог принятия решения, а $ \langle \cdot, \cdot \rangle $ -- скалярное произведение.
\newline
Задача обучения линейного классификатора заключается в том, чтобы по выборке настроить вектор весов. Для этого в логистической регрессии решается задача минимизации эмпирического риска с специальной функцией потерь вида:
\[
Q(w) = \sum_{i=1}^{m}ln(1+e^{-y_i\langle x_i,w\rangle}) \to \min_{w}
\]
После нахождения решения $w$, становится возможным не только вычислять классификацию для произвольного объекта, но и оценивать апостериорные вероятности его принадлежности классам:
\[
\mathbb{P}(y|x) = \sigma(y\langle x,w\rangle)
\]
где $\sigma(z) = \frac{1}{1+e^{-z}}$ -- сигмоидная функция.


\subsection{Метод опорных векторов}
Метод опорных векторов -- также очень популярный метод машинного обучения, достаточно мощный и многогранный, применяемый в задачах классификации и регрессии. 
\newline
Фундаментальная идея метода опорных векторов заключается в поиске гиперплоскости с наилучшим отступом -- расстоянием между гиперплоскостью и опорными векторами -- векторами, которые ближе всего находятся к разделяющей гиперплоскости. 
\newline
Ищется решение задачи регрессии в линейном случае: 
\[
f(x) = \langle w,x\rangle - w_0.
\] 
Функция потерь принимает вид:
\[
a(x_i) = \abs{\langle w,x_i\rangle - w_0 - y_i}_{\varepsilon} 
\]
для каждого вектора $(x_i,y_i)$.
\newline
В таком случае функционал потерь принимает вид:
\[
Q_\varepsilon(a,X) = \sum_{i=1}^{l}\abs{\langle w,x_i\rangle - w_0-y_i}_\varepsilon + \tau\langle w,w\rangle^2 \to \min_{w,w_0}.
\]
Последнее слагаемое удерживает коэффициенты $w$ от бесконечного возрастания. Аналогично задаче классификации, решение зависит от скалярного произведения объектов, а не от самих объектов. Минимизация в данном случае эквивалентна задаче квадратичного программирования с ограничениями типа неравенств. 

\iffalse
Фундаментальная идея метода опорных векторов заключается в поиске гиперплоскости с "лучшим" отступом. расстоянием между гиперплоскостью и опорными векторами -- векторами, которые ближе всего находятся к разделяющей гиперплоскости. 
\newline
Если выборка линейно разделима, то гиперплоскость, разделяющая два класса, имеет вид:
\[
\langle x,w\rangle + b = 0
\]
Для надежного разделения классов необходимо, чтобы расстояние между разделяющими гиперплоскостями было как можно большим, т.е. $\norm{w}$ было как можно меньше. Таким образом ставится задача нахождения минимума квадратичного функционала $ \frac{\langle w,w\rangle}{2} $.
\newline
Если же выборка линейно неразделима, то применяется т.н. ядерный трюк -- пространство признаков вкладывается в пространство большей размерности $H$ с помощью отображения $\varphi: X \to H$. Полагают, что $H$ -- гильбертово пространство, и тогда, рассматривая алгоритм опорных векторов для образов $\varphi(x_i)$ решение задачи сводится к линейно разделимому случаю, т.е. разделяющая функция ищется в виде:
\[
f(x) = \langle w,\varphi(x)\rangle + b
\]
Неоспоримым плюсом этого метода является то, что он сводится к решению задачи квадратичного программирования в выпуклой области, которая всегда имеет единственное решение.\fi 

\subsection{Градиентный бустинг}
Градиентный бустинг — это метод машинного обучения, посвященный решению для задач классификации и регрессии, которая строит модель предсказания в форме ансамбля слабых предсказывающих моделей, обычно деревьев решений. Обучение
ансамбля проводится последовательно в отличие, например от бэггинга. На
каждой итерации вычисляются отклонения предсказаний уже обученного ансамбля на обучающей выборке. Следующая модель, которая будет добавлена в ансамбль будет предсказывать эти отклонения. Таким образом, добавив
предсказания нового дерева к предсказаниям обученного ансамбля мы можем
уменьшить среднее отклонение модели, котрое является таргетом оптимизационной задачи. Новые деревья добавляются в ансамбль до тех пор, пока ошибка
уменьшается, либо пока не выполняется одно из правил ранней остановки.


\subsection{Случайный лес}
Случайный лес -- алгоритм машинного обчения, заключающийся в использовании ансамбля решающих деревьев. Алгоритм применяется как для задач классификации, так и для задач регрессии или кластеризации. Каждое решающее дерево само по себе дает не очень высокое качество классификации, но за счет их большого количества результат выходит хорошим.
\newline
Наиболее распространенный способ построения деревьев -- бэггинг. Идея заключается в генерации случайной подвыборки с повторениями. Далее строится решающее дерево, которое классифицирует образцы данной подвыборки, причем в ходе создания очередного узла дерева выбирается набор признаков, на основе которых производится разбиение, из которых выбирается наилучший(например, с использованием критерия Джини). Дерево строится до полного исчерпания подвыборки.
\newline
В задачах регрессии предсказывается значение путем усреднения ответов деревьев.