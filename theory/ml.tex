\section{Элементы машинного обучения} % cf my ml writings

\subsection{Логистическая регрессия}
Логистическая регрессия --один из самых простых и популярных алгоритмов для задачи классификации. Смысл данного метода заключается в том, чтобы построить разделяющую гиперплоскость в пространстве признаков, позволяющую отделить классы друг от друга. 
\newline
В логистической регрессии строится линейный алгоритм классификации $ a:X \to Y $, где $X$ -- пространство признаков, а $Y$ -- конечное множество номеров классов. Алгоритм имеет вид:
\[
a(x,w) = sign(\sum_{j=1}^{n}w_jf_j(x)-w_0) = sign<x,w>,
\]
где $w$ -- вектор весов, $w_0$ -- порог принятия решения, а $ <\cdot, \cdot> $ -- скалярное произведение.
\newline
Задача обучения линейного классификатора заключается в том, чтобы по выборке настроить вектор весов. Для этого в логистической регрессии решается задача минимизации эмпирического риска с специальной функцией потерь вида:
\[
Q(w) = \sum_{i=1}^{m}ln(1+e^{-y_i<x_i,w>}) \to \min_{w}
\]
После нахождения решения $w$, становится возможным не только вычислять классификацию для произвольного объекта, но и оценивать апостериорные вероятности его принадлежности классам:
\[
\mathbb{P}(y|x) = \sigma(y<x,w>)
\]
где $\sigma(z) = \frac{1}{1+e^{-z}}$ -- сигмоидная функция.


\subsection{Метод опорных векторов}
Метод опорных векторов -- также очень популярный метод машинного обучения, достаточно мощный и многогранный, применяемый в задачах классификации и регрессии. 
\newline
Фундаментальная идея метода опорных векторов заключается в поиске гиперплоскости с наилучшим отступом -- расстоянием между гиперплоскостью и опорными векторами -- векторами, которые ближе всего находятся к разделяющей гиперплоскости. 
\newline
Ищется решение задачи регрессии в линейном случае: 
\[
f(x) = <w,x> - w_0.
\] 
Функция потерь принимает вид:
\[
a(x_i) = \abs{<w,x_i> - w_0 - y_i}_{\varepsilon} 
\]
для каждого вектора $(x_i,y_i)$.
\newline
В таком случае функционал потерь принимает вид:
\[
Q_\varepsilon(a,X) = \sum_{i=1}^{l}\abs{<w,x_i> - w_0-y_i}_\varepsilon + \tau<w,w>^2 \to \min_{w,w_0}.
\]
Последнее слагаемое удерживает коэффициенты $w$ от бесконечного возрастания. Аналогично задаче классификации, решение зависит от скалярного произведения объектов, а не от самих объектов. Минимизация в данном случае эквивалентна задаче квадратичного программирования с ограничениями типа неравенств. 

\iffalse
Фундаментальная идея метода опорных векторов заключается в поиске гиперплоскости с "лучшим" отступом. расстоянием между гиперплоскостью и опорными векторами -- векторами, которые ближе всего находятся к разделяющей гиперплоскости. 
\newline
Если выборка линейно разделима, то гиперплоскость, разделяющая два класса, имеет вид:
\[
<x,w> + b = 0
\]
Для надежного разделения классов необходимо, чтобы расстояние между разделяющими гиперплоскостями было как можно большим, т.е. $\norm{w}$ было как можно меньше. Таким образом ставится задача нахождения минимума квадратичного функционала $ \frac{<w,w>}{2} $.
\newline
Если же выборка линейно неразделима, то применяется т.н. ядерный трюк -- пространство признаков вкладывается в пространство большей размерности $H$ с помощью отображения $\varphi: X \to H$. Полагают, что $H$ -- гильбертово пространство, и тогда, рассматривая алгоритм опорных векторов для образов $\varphi(x_i)$ решение задачи сводится к линейно разделимому случаю, т.е. разделяющая функция ищется в виде:
\[
f(x) = <w,\varphi(x)> + b
\]
Неоспоримым плюсом этого метода является то, что он сводится к решению задачи квадратичного программирования в выпуклой области, которая всегда имеет единственное решение.\fi 

\subsection{Градиентный бустинг}
Градиентный бустинг — это метод машинного обучения, посвященный решению для задач классификации и регрессии, которая строит модель предсказания в форме ансамбля слабых предсказывающих моделей, обычно деревьев решений. Обучение
ансамбля проводится последовательно в отличие, например от бэггинга. На
каждой итерации вычисляются отклонения предсказаний уже обученного ансамбля на обучающей выборке. Следующая модель, которая будет добавлена в ансамбль будет предсказывать эти отклонения. Таким образом, добавив
предсказания нового дерева к предсказаниям обученного ансамбля мы можем
уменьшить среднее отклонение модели, котрое является таргетом оптимизационной задачи. Новые деревья добавляются в ансамбль до тех пор, пока ошибка
уменьшается, либо пока не выполняется одно из правил ранней остановки.
\iffalse
Данная модель имеет несколько гиперпараметров, которые необходимо подбирать для каждой конкретной задачи заново, вот некоторые из них:
\begin{itemize}
	\item $n\_estimators$ — число деревьев
	\item $max\_depth$ -- максимальная глубина дерева
	\item $learning\_rate$ -- скорость обучения
\end{itemize}\fi

\subsection{Случайный лес}
Случайный лес -- алгоритм машинного обчения, заключающийся в использовании ансамбля решающих деревьев. Алгоритм применяется как для задач классификации, так и для задач регрессии или кластеризации. Каждое решающее дерево само по себе дает не очень высокое качество классификации, но за счет их большого количества результат выходит хорошим.
\newline
Наиболее распространенный способ построения деревьев -- бэггинг. Идея заключается в генерации случайной подвыборки с повторениями. Далее строится решающее дерево, которое классифицирует образцы данной подвыборки, причем в ходе создания очередного узла дерева выбирается набор признаков, на основе которых производится разбиение, из которых выбирается наилучший(например, с использованием критерия Джини). Дерево строится до полного исчерпания подвыборки.
\newline
В задачах регрессии предсказывается значение путем усреднения ответов деревьев.